{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sam L - final_release.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdlanda77/portfolio/blob/main/Sam_L_final_release.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjnUTgMYK-b9"
      },
      "source": [
        "# Problem 1 (10 points)\n",
        "\n",
        "Recall the word ladder puzzle from Homework 1. Suppose we change the rules so that either changing from or changing to a single vowel counts as two letter changes. An optimal solution is still one that minimizes the number of letter changes, but now with vowels counting double.\n",
        "\n",
        "1. Suppose all ties are broken in the same way and that any heuristics used remain the same as in the original word ladder problem. For which search algorithms is it possible that a different solution is returned compared to the original problem? Consider DFS, BFS, IDS, and A\\*, and briefly explain your answer for each.\n",
        "\n",
        "2. Suppose all ties are broken in the same way and that any heuristics used remain the same as in the original word ladder problem. For which search algorithms does the guarantee of solution optimality change? Consider DFS, BFS, IDS, and A\\*, and briefly explain your answer for each.\n",
        "\n",
        "3. Is Hamming distance still an admissible heuristic for the new problem? Briefly explain your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUwHNvjNMaM1"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\n",
        "\n",
        "1. DFS would return the same solution as the original problme, becasue decisions are made based solely on depth, which is not changing. While cost of some letter changes is changing, DFS does not consider cost, so there will be no change in the returned solution. \n",
        "\n",
        "BFS would return the same solution as well. Similalry, it does not account for cost, instead decisions are taken based on which node is the shallowest. Again, since dpeth is not changing, the order of node expanded will not be affected by the cost change, so there is no change in the returned solution.\n",
        "\n",
        "IDS does not consider cost either, so the returned solution will be the same. Like DFS, decisions are made based on depth, which is not changing so there will be no change in the returned solution.\n",
        "\n",
        "Since A* considers the sum of cost to a node and the heuristic function when decisions are made, it is possible that a different solution is returned when A* search is used on the problem with altered cost.\n",
        "\n",
        "\n",
        "\n",
        "2. DFS, as in the original problem, does not guarentee optimality. DFS returns the first solution it finds, which is not guarenteed to be optimal since the the optimal solution is likely not on the first path that DFS traverses due to the fact that DFS traverses to the max depth first. \n",
        "\n",
        "BFS is only guarenteed to return an optimal solution when costs are uniform, since cost and depth would directly proportional in this case. In the new problem with weighted vowels, this does not hold so BFS can no longer guarentee and optimal solution. \n",
        "\n",
        "IDS is no longer guarenteed to return an optimal solution. Following similar logic as BFS, IDS returns an optimal path when costs are unniform, but with the inconsistent weighting in the new problem, there is no guarentee that an optimal path is returned since cost and depth are no longer directly proportional.\n",
        "\n",
        "A* search returns an optimal solution as long as the heuristic is admissible. So it would still guarentee returning an optimal solution since the Hamming Distance heuristic is admissible. \n",
        "\n",
        "3. Hamming distance is still an admissible heuristic for the new problem. Since hamming distance considers the \"net depth\" or the distance from node n to the goal, this value will never be more than the cost since the cost of each additional depth is at least 1. Therefore, it still qualifies as an admissible heuristic since it will not overestimate the true cost. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osyq0--xMqjV"
      },
      "source": [
        "# Problem 2 (15 points)\n",
        "\n",
        "X and O are playing 3x3 tic-tac-toe again, but they no longer consider the game to be zero-sum. Each player now intends to maximize their own utilities, regardless of what utility the other player receives. A ```terminal``` function takes in a game state and returns a) whether the state is a terminal state, and b) a *list* of two utilities for each player (X is first, O is second) if it is terminal. The new function signature is as follows:\n",
        "\n",
        "```\n",
        "def terminal(state):\n",
        "  INPUT: A game state\n",
        "  OUTPUTS: Boolean indicating whether state is terminal\n",
        "           List of two utilities [X_utility, O_utility] if terminal, or None\n",
        "```\n",
        "\n",
        "We can recursively find the value of a state in this new game as a function of the current player (a ```'X'``` or ```'O'``` string), similar to the implementation in minimax or alpha-beta. Complete the ```value``` function below to achieve this. It should contain a recursive call to itself with the opposing player; there is no pruning here. You may assume that we still have the same ```actions``` and ```result``` functions from Homework 2, with signatures as follows:\n",
        "\n",
        "```\n",
        "def actions(state):\n",
        "  INPUT: A game state\n",
        "  OUTPUTS: List of possible actions in state\n",
        "\n",
        "def result(state, player, action):\n",
        "  INPUTS: A game state, player, and action\n",
        "  OUTPUT: New game state as a result of player taking action from state\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AToluZq8PPHR"
      },
      "source": [
        "def value(state, player):\n",
        "  isTerminal, score = terminal(state)\n",
        "  if isTerminal:\n",
        "    return score, None\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  if player == 'X':\n",
        "    other_player = 'O'\n",
        "    i = 0\n",
        "  else:\n",
        "    other_player = 'X'\n",
        "    i = 1\n",
        "  score = [-float(\"inf\"), -float(\"inf\")]\n",
        "  act = None\n",
        "  \n",
        "  for a in actions(state):\n",
        "    old_score = score[i]\n",
        "    score[i] = max(score[i], value(result(state, player, a), other_player)[0][i])\n",
        "    if old_score < score[i]:\n",
        "      act = a\n",
        "  return score, act\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF6oNrSzsbjE"
      },
      "source": [
        "You can check that your implementation above is free of syntactical and logical errors by running the unit test below (note this does *not* give any indication of the correctness of your implementation). We cannot answer questions on what the expected output should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pYqZlMfs-Sl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "07044d0e-6d06-4799-8ece-733a4238cf74"
      },
      "source": [
        "def terminal(s):\n",
        "  if s == 's1': return False, None\n",
        "  elif s == 's2': return True, [1,-2]\n",
        "def actions(s):\n",
        "  return ['a1','a2']\n",
        "def result(s,p,a):\n",
        "  return 's2'\n",
        "\n",
        "print(value('s1','X'))\n",
        "print(value('s2','X'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "([1, -inf], 'a1')\n",
            "([1, -2], None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhuewOeyQp4K"
      },
      "source": [
        "# Problem 3 (15 points)\n",
        "\n",
        "Dynamic programming can be used to solve a MDP when the model is known. We saw that these methods produced optimal value functions and policy functions. It turns out that we can also use *Q-value iteration* to find Q-values when the model is known. Like state values, Q-values can be defined recursively with respect to each other in the form of a Bellman equation. For dynamic programming, a Q-value can be updated as follows:\n",
        "\n",
        "$$ Q_{i+1}(s,a) \\leftarrow \\sum_{s'} T(s,a,s')[R(s,a,s') + \\gamma \\max_{a'} Q_i(s',a')] $$\n",
        "\n",
        "Implement the Q-value iteration function below. It takes in an initial set of Q-values in the form of a ```{(state, action): value}``` dictionary, a discount factor ```gamma```, and a ```threshold```. Like value iteration, Q-value iteration stops as soon as the maximum change from one iteration to the next is no larger than ```threshold```.\n",
        "\n",
        "You may reference the following variables and functions in your implementation:\n",
        "\n",
        "*  ```STATES```: A list of states in the MDP.\n",
        "\n",
        "*  ```ACTIONS```: A list of actions in the MDP, same for all states.\n",
        "\n",
        "*  ```T(state, action, successor_state)```: A transition function, with inputs as shown.\n",
        "\n",
        "*  ```R(state, action, successor_state)```: A reward function, with inputs as shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPsJniMuDfIT"
      },
      "source": [
        "def Qvalue_iteration(initial, gamma, threshold):\n",
        "  Qvalues = initial\n",
        "  # YOUR CODE HERE\n",
        "  delta = float(\"inf\")\n",
        "  while delta > threshold:\n",
        "    diff = []\n",
        "    newQvalues = {(s,a):0 for s in STATES for a in ACTIONS}\n",
        "    for pair in Qvalues.keys():\n",
        "      sum = 0\n",
        "      for state in STATES:\n",
        "        max_a = -float(\"inf\")\n",
        "        for action in ACTIONS:\n",
        "          max_a = max(max_a, Qvalues.get((state, action)))\n",
        "        #print(max_a)\n",
        "        sum += (T(pair[0], pair[1], state)*(R(pair[0], pair[1], state) + gamma*max_a))\n",
        "      newQvalues[pair] = sum\n",
        "      diff.append(abs(Qvalues[pair] - newQvalues[pair]))\n",
        "    delta = max(diff)\n",
        "    #print(delta)\n",
        "    Qvalues = newQvalues\n",
        "    #print(Qvalues)\n",
        "\n",
        "  return Qvalues"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CnvxuufwsTS"
      },
      "source": [
        "You can check that your implementation above is free of syntactical and logical errors by running the unit test below (note this does *not* give any indication of the correctness of your implementation). We cannot answer questions on what the expected output should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWRt2puGfD4x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "3742de8e-2d40-43eb-c899-12c1f426e94a"
      },
      "source": [
        "STATES = ['s1', 's2']\n",
        "ACTIONS = ['a1', 'a2']\n",
        "\n",
        "def T(s, a, sp):\n",
        "  if s == 's1' and a == 'a1' and sp == 's2': return 1\n",
        "  elif s == 's1' and a == 'a2' and sp == 's1': return 1\n",
        "  elif s == 's2' and a == 'a2' and sp == 's1': return 1\n",
        "  elif s == 's2' and a == 'a1' and sp == 's2': return 1\n",
        "  else: return 0\n",
        "def R(s, a, sp):\n",
        "  if sp == 's1': return -1\n",
        "  elif sp == 's2': return 1\n",
        "\n",
        "Qvalue_iteration({(s,a):0 for s in STATES for a in ACTIONS}, .5, 1e-3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('s1', 'a1'): 1.9990234375,\n",
              " ('s1', 'a2'): -0.0009765625,\n",
              " ('s2', 'a1'): 1.9990234375,\n",
              " ('s2', 'a2'): -0.0009765625}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuCoInshppF6"
      },
      "source": [
        "# Problem 4 (20 points)\n",
        "\n",
        "Recall that the forward algorithm for a HMM iteratively computes the marginal distribution over each state variable conditioned on all evidence thus far: $\\Pr(X_t \\mid e_{1:t})$.\n",
        "\n",
        "Suppose we are now interested in iteratively computing a *joint* distribution over the *two* most recent states given all evidence thus far: $g_t = \\Pr(X_t, X_{t-1} \\mid e_{1:t})$. That is, we want to start with the aforementioned distribution and obtain $g_{t+1} = \\Pr(X_{t+1}, X_t \\mid e_{1:t+1})$.\n",
        "\n",
        "Write a mathematical expression indicating how we can obtain $g_{t+1}$ from $g_t$. All quantities in the expression should be known or found in the HMM model. You can give an unnormalized expression instead of one that is exactly equal to $\\Pr(X_{t+1}, X_t \\mid e_{1:t+1})$; if so, please use the LaTeX symbol \"\\propto\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHnOA1GaC5oa"
      },
      "source": [
        "\n",
        "From $g_{t}$, we first sum out $X_{t-1}$, which will give us $P(X_{t} | e_{1:t})$, which we then multiply by the transition matrix and the observation matrix, which are $P(X_{t+1} | X_{t})$ and $P(e_{t+1} | X_{t+1})$ repsectively. This gives the expression:\n",
        "\n",
        "$$ g_{t+1} \\propto (\\sum_{X_{t-1}}g_{t})P(X_{t+1}|X_{t})P(e_{t+1} | X_{t+1})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdOkFQSmEjoP"
      },
      "source": [
        "Use the expression that you came up with above to implement ```joint_dist_update```. Given $g_t$, the observation matrix corresponding to the next timestep, and the HMM transition model, compute and return $g_{t+1}$. Joint distributions are represented as 2D numpy arrays, in the same way as a transition matrix. The function inputs are as follows.\n",
        "\n",
        "*  ```gt``` is a 2D numpy array, where ```gt[i,j]``` is the probability $\\Pr(X_t=i, X_{t-1}=j \\mid e_{1:t})$. \n",
        "\n",
        "*  ```Tprobs``` is a 2D numpy array, where ```Tprobs[i,j]``` is the probability $\\Pr(X_{t+1} = i \\mid X_{t} = j)$ (transition matrix).\n",
        "\n",
        "*  ```Oprobs``` is a 1D numpy array, where ```Oprobs[i]``` is the probability $\\Pr(e_{t+1} \\mid X_{t+1} = i)$ (observation probabilities). \n",
        "\n",
        "Make sure that your return value is also a joint distribution in the form of a 2D numpy array and that it is properly normalized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUmispu48d1m"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def joint_dist_update(gt, Tprobs, Oprobs):\n",
        "  # YOUR CODE HERE\n",
        "  #print(gt)\n",
        "  gt_next = np.zeros(len(gt))\n",
        "  for i in range(len(gt)):\n",
        "    gt_next[i] = np.sum(gt[i])\n",
        "  gt_next = gt_next*Tprobs\n",
        "  gt_next = gt_next*Oprobs\n",
        "  sum = np.sum(gt_next)\n",
        "  gt_next = gt_next/sum\n",
        "  return gt_next\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1KFErZRABW2"
      },
      "source": [
        "You can check that your implementation above is free of syntactical and logical errors by running the unit test below (note this does *not* give any indication of the correctness of your implementation). We cannot answer questions on what the expected output should be."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJoAKIgk-39e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "75ae91d5-89d0-4bc5-f58d-f7e3411efd4f"
      },
      "source": [
        "print(joint_dist_update(np.array([[.3,.1],[.4,.2]]), \n",
        "                        np.array([[.2, .5], [.8, .5]]), np.array([.1,.4])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.02857143 0.42857143]\n",
            " [0.11428571 0.42857143]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}