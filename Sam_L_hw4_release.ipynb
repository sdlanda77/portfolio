{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sam L - hw4_release.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdlanda77/portfolio/blob/main/Sam_L_hw4_release.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkhCnnA-32fP"
      },
      "source": [
        "# Problem 1 (40 points)\n",
        "\n",
        "A robot is wandering around a room with some obstacles, labeled as $\\#$ in the grid below. It can occupy any of the free cells labeled with a letter, keeping a belief distribution over its current location. At each timestep it moves from its current cell to a neighboring free cell in one of the four cardinal directions with uniform probability; it cannot stay in the same cell. For example, from A the robot can move to either B or D with probability $\\frac12$, while from B it can move to A, C, or E, each with probability $\\frac13$.\n",
        "\n",
        "![robot-grid.PNG](data:image/png;base64,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)\n",
        "\n",
        "The robot also makes an observation after each transition, returning what it sees in a randomly chosen cardinal direction. Possibilities include observing $#$, \"wall\", or \"empty\" (for a free cell). For example, in A the robot observes \"wall\" and \"empty\", each with probability $\\frac12$; in C the robot observes \"wall\" with probability $\\frac12$, \"empty\" $\\frac14$, and $\\#$ $\\frac14$.\n",
        "\n",
        "**Note**: You don't have to show work for solving linear equations or eigenvectors, but please show what equations or matrices you use. Feel free to show your work in Python as well. You may also omit computations that will turn out to be zero based on the provided information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBwPiioasIfZ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "1.  Suppose that the robot wanders around forever without making observations. What is the stationary distribution over the robot's predicted location? \n",
        "\n",
        "2. The initial distribution $X_0$ is uniform over all possible states. The robot makes a transition and observes $e_1 = \\#$. It then makes a second transition and again observes $e_2 = \\#$. What are the belief distributions $\\Pr(X_1 \\mid e_1)$ and $\\Pr(X_2 \\mid e_1, e_2)$?\n",
        "\n",
        "4. Compute the joint distribution $\\Pr(X_1, X_2 \\mid e_1, e_2)$. Hint: First determine the state sequences with a nonzero probability.\n",
        "\n",
        "5. Are the states $X_1$ and $X_2$ independent given $e_1$ and $e_2$? Why or why not? What is the most likely state sequence of $X_1$ and $X_2$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_c8qZSyuGcJ"
      },
      "source": [
        "ENTER YOUR RESPONSES HERE\n",
        "\n",
        "1.   T is a transition matrix for the grid, states A - F.\n",
        "          T = [[0,0.33,0,0.5,0,0],\n",
        "              [0.5,0,1,0,0.33,0],\n",
        "              [0,0.33,0,0,0,0],\n",
        "              [0.5,0,0,0,0.33,0],\n",
        "              [0,0.33,0,0.5,0,1],\n",
        "              [0,0,0,0,0.33,0]]\n",
        "\n",
        "The stationary distribution is represented as a vector v: $$<a,b,c,d,e,f>$$ With $\\lambda = 1$ we have $Tv=1*v$, which gives the equations: $$ a = 0.33b + 0.5d, b=0.5a + c +0.33e, c = 0.33b, d=0.5a+0.33e, e=0.33b+0.5d+f, f= 0.33e $$ Since v is a transition matrix, all elements will sum to 1, or $$a+b+c+d+e+f=1$$ We solve the linear equation and find that $$v= <1/6,1/4,1/12,1/6,1/4,1/12>$$\n",
        "\n",
        "\n",
        "2. $f_{t} = P(X_{t} | e_{1:t})$\n",
        "\n",
        "Therefore, we are looking for $f_{1}$ and $f_{2}$. $f_{0}=P(X_{0})=<1/6,1/6,1/6,1/6,1/6, 1/6>$ and $T$ is the same as problem 1. $$f'_{1} = Tf_{0} = <5/36,11/36,2/36,5/36,11/36,2/36>$$ $$O_{1}f'_{1} = <0,0,1/72,5/144,11/144,1/36>$$ Divide by $11/72$ to normalize: $$f_{1} = P(X_{1} | e_{1}) = <0,0,1/11,5/22,1/2,2/11>$$\n",
        "\n",
        "$$f'_{2} = Tf_{1} = <5/44,17/66,0,1/6,13/44,1/6>$$ $$O_{2}f'_{2} = <0,0,0,1/24,13/176,1/12>$$ Divide by $35/176$ to normalize: $$f_{2}= P(X_{2} | e_{1:2}) = <0,0,0,22/108,13/35,44/105>$$\n",
        "\n",
        "3. $P(X_{1}, X_{2} | e_{1}, e_{2}) = \\frac{P(X_{1}, X_{2},e_{1}, e_{2})}{P(e_{1}, e_{2})}$. The denominator will be a constant, so we ignore it here and normalize at the end. State sequences with nonzero probability given $e_{1} = e_{2} = # $: (D,E), (E,D), (E,F), (F,E). We can calculate the probabilities of these specific pairs using the distributions.\n",
        "\n",
        "$P(X_{1} = D, X_{2} = E,e_{1}, e_{2}) = P(X_{1} = D | e_{1}) * P(X_{2} = E| e_{1:2})$. From above we get: $5/22 * 13/35 = 13/154$\n",
        "\n",
        "$P(X_{1} = E, X_{2} = D,e_{1}, e_{2}) = P(X_{1} = E | e_{1}) * P(X_{2} = D| e_{1:2})$. From above we get: $ 1/2 * 22/108 = 11/108$\n",
        "\n",
        "$P(X_{1} = E, X_{2} = F,e_{1}, e_{2}) = P(X_{1} = E | e_{1}) * P(X_{2} = F| e_{1:2})$. From above we get: $1/2 * 44/105 = 22/105$\n",
        "\n",
        "$P(X_{1} = F, X_{2} = E,e_{1}, e_{2}) = P(X_{1} = F | e_{1}) * P(X_{2} = E| e_{1:2})$. From above we get: $2/11 * 13/35 = 26/385$\n",
        "\n",
        "Therefore $P(X_{1}, X_{2},e_{1}, e_{2})$ is given by the 6x6 matrix A where $A_{ùëñùëó}=ùëÉ(ùëã_{1}  = j, ùëã_{2} =i, e_{1}, e_{2})$ and (in other words, the row is the first state and the column is the second): \n",
        "          \n",
        "          P = [[0,0,0,0,0,0],\n",
        "              [0,0,0,0,0,0],\n",
        "              [0,0,0,0,0,0],\n",
        "              [0,0,0,0,13/154,0],\n",
        "              [0,0,0,11/108,0,22/105],\n",
        "              [0,0,0,0,26/385,0]]\n",
        "\n",
        "Normalize the matrix (divide each element by 0.46):\n",
        "\n",
        "          P = [[0,0,0,0,0,0],\n",
        "              [0,0,0,0,0,0],\n",
        "              [0,0,0,0,0,0],\n",
        "              [0,0,0,0,0.18,0],\n",
        "              [0,0,0,0.22,0,0.45],\n",
        "              [0,0,0,0,0.15,0]]\n",
        "\n",
        "4. No, $X_{1}$ and $X_{2}$ are not independent given $e_{1}$ and $e_{2}$ becasue the distibution for $X_{2}$ is calculated following the distribution for $X_{1}$. Furthermore, $P(X_{2} | X_{1}) = T*(T*P(X_{0}))$ and $P(X_{2} | X_{1}, e_{1}, e_{2}) = f_{2}$ from above. Since the first element of $f_{2}$ is zero, we just need to know that the first element of $T*(P(X_{0})*T$ is nonzero to prove they are not independent. $T*P(X_{0} = Tf_{0} = <5/36,11/36,2/36,5/36,11/36,2/36>$ from above and $T(Tf_{0})$ will therefore have the first element $37/216$. Since these two instances are not equal, $X_{1}$ and $X_{2}$ are not independent given $e_{1}$ and $e_{2}$.\n",
        "\n",
        "The most likely state sequence is (E,F), since that is the maximum element is the array above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgTXtGN4Ojtm"
      },
      "source": [
        "# ENTER ANY CODE FOR PROBLEM 1 HERE\n",
        "\n",
        "import numpy as np\n",
        "#done by hand\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQMAqpGGUu2g"
      },
      "source": [
        "# POS Tagging\n",
        "\n",
        "In this assignment you will explore [part-of-speech (POS) tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging), a standard task in natural language processing. The goal is to identify parts of speech and related labels for each word in a given corpus. HMMs are well suited for this problem, with parts of speech being hidden states and the words themselves being observations.\n",
        "\n",
        "We will be using data from the English EWT treebank from [Universal Dependencies](https://universaldependencies.org/treebanks/en_ewt/index.html), which uses 17 POS tags. We are providing clean versions of training and test data for you. The data format is such that each line contains a word and associated tag, and an empty lines signifies the end of a sentence. Feel free to open the files in a text editor to get an idea.\n",
        "\n",
        "Start by uploading both files to the Jupyter session storage (you should do this each time that you start a new session). Then run the following code cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8YbOgtrrymZ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def read_sentence(f):\n",
        "  sentence = []\n",
        "  while True:\n",
        "    line = f.readline()\n",
        "    if not line or line == '\\n':\n",
        "      return sentence\n",
        "    line = line.strip()\n",
        "    word, tag = line.split(\"\\t\", 2)\n",
        "    sentence.append((word, tag))\n",
        "\n",
        "def read_corpus(file):\n",
        "  f = open(file, 'r', encoding='utf-8')\n",
        "  sentences = []\n",
        "  while True:\n",
        "    sentence = read_sentence(f)\n",
        "    if sentence == []:\n",
        "      return sentences\n",
        "    sentences.append(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m4b10sfi4WC"
      },
      "source": [
        "training = read_corpus('train.upos.tsv')\n",
        "TAGS = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', \n",
        "        'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
        "NUM_TAGS = len(TAGS)\n",
        "\n",
        "alpha = 0.1\n",
        "tag_counts = np.zeros(NUM_TAGS)\n",
        "transition_counts = np.zeros((NUM_TAGS,NUM_TAGS))\n",
        "obs_counts = {}\n",
        "\n",
        "for sent in training:\n",
        "  for i in range(len(sent)):\n",
        "    word = sent[i][0]\n",
        "    pos = TAGS.index(sent[i][1])\n",
        "    tag_counts[pos] += 1\n",
        "    if i < len(sent)-1:\n",
        "      transition_counts[TAGS.index(sent[i+1][1]), pos] += 1\n",
        "    if word not in obs_counts:\n",
        "      obs_counts[word] = np.zeros(NUM_TAGS)\n",
        "    (obs_counts[word])[pos] += 1\n",
        "\n",
        "X0 = tag_counts / np.sum(tag_counts)\n",
        "TPROBS = transition_counts / np.sum(transition_counts, axis=0)\n",
        "OPROBS = {'#UNSEEN': alpha*np.ones(NUM_TAGS)}\n",
        "for word, counts in obs_counts.items():\n",
        "  OPROBS[word] = np.divide(counts, tag_counts+alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W-eqo75bsxw"
      },
      "source": [
        "The preceding cell estimates the parameters of the HMM model by going through all the training data and counting each tag, word, and transition that appears. ```X0``` is a numpy array storing the initial distribution of tags. ```TPROBS``` is the transition matrix, in the form of a 2d numpy array. ```OPROBS``` is a dictionary of 1d numpy arrays, with keys as the words appearing in the training data and their values being 1d numpy arrays of the emission probabilities. \n",
        "\n",
        "Notice that we're including one extra \"tag\" in ```OPROBS```: an ```#UNSEEN``` tag. This is necessary because we will inevitably encounter words in the test dataset that we have not seen before. For any word that we have not seen, we treat it as if the word is just ```#UNSEEN```. We assign the \"count\" of an ```#UNSEEN``` word to a constant value $\\alpha$, so that it acts as Laplacian smoothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmfThHKofcPQ"
      },
      "source": [
        "## Coding 1 (5 points)\n",
        "\n",
        "Before we build our HMM, let's try a simplistic unigram model: given a new word, assign it the POS tag giving it the highest emission probability. Context of surrounding words is therefore not considered. This can be done by taking the ```argmax``` of the emission probabilities for the given word in OPROBS. Remember that we treat all words that did not appear before as ```#UNSEEN```. Complete the function below to achieve this (make sure to actually return the POS tag itself, not the tag index)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhZsS0vfoC7t"
      },
      "source": [
        "def unigram(obs):\n",
        "  # Returns the tag of the word obs, as predicted by a unigram model\n",
        "  # YOUR CODE HERE\n",
        "  \n",
        "  value = OPROBS.get(obs)\n",
        "  tag_index = NUM_TAGS - 1\n",
        "  if value is None:\n",
        "    value = OPROBS.get('#UNSEEN')\n",
        "  \n",
        "  tag_index = np.argmax(value)\n",
        "  return TAGS[tag_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5IuwxZmhGx_"
      },
      "source": [
        "Test out your unigram model by running the cell below, which will tag the specified data and compute accuracy rates over all words and unseen words only. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_28iq5OhHC5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "3cc55886-f5c9-4331-96e7-c72cbfc72071"
      },
      "source": [
        "def evaluate(sentences, method):\n",
        "  correct = 0\n",
        "  correct_unseen = 0\n",
        "  num_words = 0\n",
        "  num_unseen_words = 0\n",
        "\n",
        "  for sentence in sentences:\n",
        "    words = [sent[0] for sent in sentence]\n",
        "    pos = [sent[1] for sent in sentence]\n",
        "    unseen = [word not in OPROBS for word in words]\n",
        "    if method == 'unigram':\n",
        "      predict = [unigram(w) for w in words]\n",
        "    elif method == 'viterbi':\n",
        "      predict = viterbi(words)\n",
        "    else:\n",
        "      print(\"invalid method!\")\n",
        "      return\n",
        "\n",
        "    if len(predict) != len(pos):\n",
        "      print(\"incorrect number of predictions\")\n",
        "      return\n",
        "    correct += sum(1 for i,j in zip(pos, predict) if i==j)\n",
        "    correct_unseen += sum(1 for i,j,k in zip(pos, predict, unseen) if i==j and k)\n",
        "    num_words += len(words)\n",
        "    num_unseen_words += sum(unseen)\n",
        "  \n",
        "  print(\"Accuracy rate on all words: \", correct/num_words)\n",
        "  if num_unseen_words > 0:\n",
        "    print(\"Accuracy rate on unseen words: \", correct_unseen/num_unseen_words)\n",
        "\n",
        "print(\"Training data evaluation\")\n",
        "evaluate(training, 'unigram')\n",
        "test = read_corpus('test.upos.tsv')\n",
        "print(\"\")\n",
        "print(\"Test data evaluation\")\n",
        "evaluate(test, 'unigram')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data evaluation\n",
            "Accuracy rate on all words:  0.9066014359235022\n",
            "\n",
            "Test data evaluation\n",
            "Accuracy rate on all words:  0.8191417300872614\n",
            "Accuracy rate on unseen words:  0.07678883071553229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuG-HJE54TfN"
      },
      "source": [
        "## Response 1 (5 points)\n",
        "\n",
        "You should see that accuracy on the training data is about 90\\%. Accuracy on the test data set is lower at about 82\\%, with accuracy on unseen words only being about 7.7\\% (only somewhat better than random chance).\n",
        "\n",
        "Why is accuracy on the training set not 100\\%, despite the model having seen the entirety of that data set before?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62lnToxylBbn"
      },
      "source": [
        "We generalize our model to avoid overfitting, in this case using additive smoothing. If we did not generalize our models, they would be a lot less useful on new testing set that strays from the patterns of the training data. This generalization is therefore necessary but means that the accuracy will only be around 90% even on training data that we've already seen in entirity. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9mf3c7ymgiN"
      },
      "source": [
        "## Coding 2 (10 points)\n",
        "\n",
        "We will now implement Viterbi to improve our performance over the unigram model. We will split the implementation into several subroutines. First complete the ```elapse_time``` function below. Given a distribution ```m```, it should return an updated \"distribution\" that occurs when applying the Viterbi update in one timestep. In addition, it should also return a list with the *indices* of the most likely prior tag for each current tag.\n",
        "\n",
        "As a hint, these will correspond to ```max``` and ```argmax``` operations, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYnTF9ke87Ku",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "331bdeb6-1220-4e5a-8826-1573aefe035b"
      },
      "source": [
        "def elapse_time(m):\n",
        "  \"\"\"\n",
        "  Given a \"message\" distribution over tags, return an updated distribution\n",
        "  after a single timestep using Viterbi update, along with a list of the \n",
        "  indices of the most likely prior tag for each current tag\n",
        "  \"\"\"\n",
        "  mprime = np.zeros(NUM_TAGS)\n",
        "  prior_tags = np.zeros(NUM_TAGS, dtype=np.int8)\n",
        "  #YOUR CODE HERE\n",
        "  \n",
        "  #find mprime\n",
        "  for i in range(TPROBS.shape[0]):\n",
        "    mprime[i] = np.max(np.multiply(TPROBS[i],m))\n",
        "  \n",
        "  #find prior_tags\n",
        "  for i in range(TPROBS.shape[0]):\n",
        "    prior_tags[i] = np.argmax(np.multiply(TPROBS[i],m))\n",
        "\n",
        "  return mprime, prior_tags\n",
        "\n",
        "print(elapse_time(X0)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.01760448 0.03555192 0.00856414 0.02886908 0.01286977 0.03107597\n",
            " 0.00131929 0.04729066 0.00355399 0.00759845 0.01917727 0.01772987\n",
            " 0.05017312 0.00486164 0.00066568 0.02484413 0.00200137]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifBYfZpruH2l"
      },
      "source": [
        "You can check your implementation above by finding $m_1'$ given $m_0 = X_0$. It should approximately look like\n",
        "```\n",
        "[0.01760448, 0.03555192, 0.00856414, 0.02886908, 0.01286977,\n",
        " 0.03107597, 0.00131929, 0.04729066, 0.00355399, 0.00759845,\n",
        " 0.01917727, 0.01772987, 0.05017312, 0.00486164, 0.00066568,\n",
        " 0.02484413, 0.00200137]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPBgLK4Cpj5w"
      },
      "source": [
        "## Coding 3 (5 points)\n",
        "\n",
        "The second step of the Viterbi algorithm is to reweight probabilities according to observations. Given the \"time elapsed\" message distribution ```mprime``` and an ```obs``` (a word), the function below should return an updated distribution weighted by the emission probabilities for this observation. Remember to account for unseen words by using the ```#UNSEEN``` word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovQm2FmpBw0v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "59b1a52d-a5be-49df-b172-9c01ec94e1dc"
      },
      "source": [
        "def observe(mprime, obs):\n",
        "  \"\"\"\n",
        "  Given a \"message\" distribution over tags, return an updated distribution\n",
        "  by weighting mprime by the emission probabilities corresponding to obs\n",
        "  \"\"\"\n",
        "  m = np.zeros(NUM_TAGS)\n",
        "  # YOUR CODE HERE\n",
        "  value = OPROBS.get(obs)\n",
        "  if value is None:\n",
        "    value = OPROBS.get('#UNSEEN')\n",
        "  m = np.multiply(value, mprime)\n",
        "\n",
        "  return m\n",
        "\n",
        "print(observe(elapse_time(X0)[0], 'multiply'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.76044808e-03 3.55519219e-03 8.56414346e-04 2.88690786e-03\n",
            " 1.28697667e-03 3.10759674e-03 1.31928571e-04 4.72906597e-03\n",
            " 3.55399416e-04 7.59844668e-04 1.91772652e-03 1.77298721e-03\n",
            " 5.01731218e-03 4.86163502e-04 6.65677893e-05 2.48441334e-03\n",
            " 2.00136540e-04]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyn5QcGgvAAh"
      },
      "source": [
        "You can check your implementation above by finding $m_1$ given $m_1'$ and an observation of your choosing. If the observation is an ```#UNSEEN``` word, you should find that $m_1$ looks something like \n",
        "```\n",
        "[1.76044808e-03, 3.55519219e-03, 8.56414346e-04, 2.88690786e-03,\n",
        "       1.28697667e-03, 3.10759674e-03, 1.31928571e-04, 4.72906597e-03,\n",
        "       3.55399416e-04, 7.59844668e-04, 1.91772652e-03, 1.77298721e-03,\n",
        "       5.01731218e-03, 4.86163502e-04, 6.65677893e-05, 2.48441334e-03,\n",
        "       2.00136540e-04]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLewyJPOrhPN"
      },
      "source": [
        "## Coding 4 (20 points)\n",
        "\n",
        "You will now put the two functions you wrote above together into one wrapper ```viterbi``` function. Given a list of word observations, it should return a corresponding list of predicted tags. As you hopefully recall, Viterbi runs in two phases. The \"forward\" phase starts with the ```X0``` distribution and goes through the observations one at a time, modifying the message distribution through time and observation updates for each. You should maintain a growing list of most likely tag pointers that is returned from each elapse time update.\n",
        "\n",
        "The second phase then goes backward. Starting with the tag with the highest likelihood at the end, follow the pointers backward to find the most likely tag for each observation. These tags should then be returned as one overall list of tags when finished (make sure the list of tags is in the right order!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xqPy45-E3hX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "003a2429-4617-424c-a874-c99743fe7468"
      },
      "source": [
        "def viterbi(observations):\n",
        "  \"\"\"\n",
        "  Given a list of word observations, return a list of predicted tags\n",
        "  \"\"\"\n",
        "  m = X0\n",
        "  pointers = []\n",
        "  # YOUR CODE HERE\n",
        "  # \"Forward\" phase of the Viterbi algorithm\n",
        "  for obs in observations:\n",
        "    elapse = elapse_time(m)\n",
        "    m = observe(elapse[0], obs)\n",
        "    pointers.append(elapse[1])\n",
        "  \n",
        "  # \"Backward\" phase of the Viterbi algorithm\n",
        "  tags = []\n",
        "  tags.append(np.argmax(m))\n",
        "  for i in range(len(pointers) - 1):\n",
        "    tags.append(pointers[len(pointers) - 1 - i][tags[i]])\n",
        "  tags.reverse()\n",
        "  for i in range(len(tags)):\n",
        "      t = TAGS[tags[i]]\n",
        "      tags[i] = t\n",
        "  return tags\n",
        "\n",
        "print(viterbi(['#UNSEEN']))  \n",
        "print(viterbi(['#UNSEEN', '#UNSEEN']))\n",
        "print(viterbi(['#UNSEEN', '#UNSEEN', '#UNSEEN']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PUNCT']\n",
            "['DET', 'NOUN']\n",
            "['ADP', 'DET', 'NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkSnP9YAv5qE"
      },
      "source": [
        "Again, we recommend that you check your implementation with small test cases. For example, running ```viterbi(['#UNSEEN'])``` (or any unseen word in place of ```#UNSEEN```) should return ```['PUNCT']```, the most likely tag from the ```observe()``` unit test above. On the other hand, running ```viterbi(['#UNSEEN', '#UNSEEN'])```, or Viterbi on two unseen words in a row, will actually return ```['DET', 'NOUN']```.\n",
        "\n",
        "## Response 2 (5 points)\n",
        "\n",
        "Explain why the tag prediction is completely different for one unseen word vs two unseen words in a row. Do the predictions make sense in the context of the English language (you can see the tag descriptions on UD's [website](https://universaldependencies.org/u/pos/))? What is the tag sequence prediction for three unseen words in a row?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrOdemDNCT7N"
      },
      "source": [
        "The tag prediction is completely different for one unseen word and two unseen words becasue the argmax(m) is the first entry in the list, and with the additional round of elapse_time() and observe() call, m is a different list. (One unseen gives argmax(m)=12 and two unseens gives argmax(m)=7.) This means there is a different tag with the highest liklihood at the end, which will result in a different traceback of the most likely sequence and therefore the tag prediction list will be different. \n",
        "\n",
        "The predictions do make sense in the context of the English language. Being that few pieces of punctuation are commonly used, it makes sense that an unseen word would likely be punctuation without any other context. The patter DET, NOUN also make sense intuitively becasue the pattern of a determiner followed by a noun is extremely common in English language (ex. a dog). Three unseens result in the pattern (ADP, DET, NOUN), which also makes sense intuitively in the Enlish language. Adposition, determinator, noun (ex. in the house) is a very common pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qsUtrNFwJai"
      },
      "source": [
        "Once you are confident of your implementation, you can run it on the full training and test data sets. You should find that both accuracy rates are better than those of the unigram model, at about 95\\% for the training set and (more importantly) 88\\% for the test set. What's more, the accuracy rate on unseen words should be about 32\\%, a very significant improvement over the unigram predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R30oE7Q9HsiJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2eedae8b-f0b5-41de-c7b9-52b2f4b88d2e"
      },
      "source": [
        "print(\"Training data evaluation\")\n",
        "evaluate(training, 'viterbi')\n",
        "print(\"\")\n",
        "print(\"Test data evaluation\")\n",
        "evaluate(test, 'viterbi')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data evaluation\n",
            "Accuracy rate on all words:  0.9527973138748919\n",
            "\n",
            "Test data evaluation\n",
            "Accuracy rate on all words:  0.883133442244093\n",
            "Accuracy rate on unseen words:  0.3267888307155323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcZCyFORyw0y"
      },
      "source": [
        "Viterbi's performance is held back by its accuracy rate on unseen words. One option is to build a higher-order HMM, e.g. a trigram model with transitions determined by the previous two states rather than one state. \n",
        "\n",
        "An alternative fix is to consider the emission probabilities of the ```#UNSEEN``` tag. As you saw in the second coding cell that learns the parameters, the ```#UNSEEN``` tag gets the same count of ```alpha``` for all POS tags. This is equivalent to saying that in a typical corpus, the distribution of unseen words is uniform across all tags. This is not a great assumption; there are probably many more nouns or verbs that we have not encountered than conjunctions or punctuation marks.\n",
        "\n",
        "Instead of a uniform distribution, it may be more reasonable to assume that the distribution of unseen words is similar to that of words that appear only once in the training set. These words are known as [*hapax legomena*](https://en.wikipedia.org/wiki/Hapax_legomenon)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi4i0FIt92ol"
      },
      "source": [
        "## Coding 5 (10 points)\n",
        "\n",
        "Complete the ```hapax_POS_counts``` function below. Given a dictionary ```obs_counts```, which maps a word to a numpy array counting the number of times it occurred as each POS tag, identify all words with a total count of 1. Add up the POS tag occurrences for each of these words, and return the total POS tag counts over all hapax legomena words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkXdM-ns2gaa"
      },
      "source": [
        "def hapax_POS_counts(obs_counts):\n",
        "  \"\"\"\n",
        "  Given a dictionary of word to count arrays, return the total number \n",
        "  of POS tag appearances only for the words with a total count of 1\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  hapax_counts = np.zeros(NUM_TAGS)\n",
        "  for count in obs_counts.values():\n",
        "    if np.sum(count) == 1:\n",
        "      hapax_counts[np.argmax(count)] += 1\n",
        "  return hapax_counts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtIl-uzzAtnM"
      },
      "source": [
        "You can check your hapax distribution by verifying that the POS tags of hapax legomena tend to be those associated with a greater diversity of words. Adjectives, verbs, nouns, and proper nouns would fall into these categories.\n",
        "\n",
        "Once you've successfully produced a distribution of hapax legomena POS tags, run the cell below. ```OPROBS``` will be recomputed using the new distribution, and Viterbi will be run again on the test data set. The accuracy rate on all words should see a slight tick up, but the accuracy on unseen words should see a marked improvement to about 47\\%. This means that we can correctly tag almost half of words that we have never encountered!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0N1ck989SEY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "eb5c4bc8-74f7-495b-f625-3c94531c9125"
      },
      "source": [
        "hapax = hapax_POS_counts(obs_counts)\n",
        "for word, counts in obs_counts.items():\n",
        "  OPROBS[word] = np.divide(counts, tag_counts + hapax)\n",
        "OPROBS['#UNSEEN'] = np.divide(hapax, tag_counts + hapax)\n",
        "\n",
        "print(\"Test data evaluation\")\n",
        "evaluate(test, 'viterbi')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data evaluation\n",
            "Accuracy rate on all words:  0.8970793321910986\n",
            "Accuracy rate on unseen words:  0.4738219895287958\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}